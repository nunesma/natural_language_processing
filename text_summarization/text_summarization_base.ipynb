{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "Text summarization is a subdomain of Natural Language Processing (NLP) that deals with extracting summaries from huge chunks of texts. <br><br>\n",
    "There are two main types of techniques used for text summarization: \n",
    "- NLP-based techniques and \n",
    "- Deep learning-based techniques. \n",
    "\n",
    "We will use in this notebook a simple NLP-based technique for text summarization. We will simply use Python's NLTK library for summarizing Wikipedia articles. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n",
    "\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from nltk.corpus import stopwords\n",
    "- from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model\n",
    "### Four steps to build a summarizer\n",
    "- 1 - Remove stop words for the analysis\n",
    "- 2 - Create frequency table of words\n",
    "- 3 - Assign score to each sentence depending on the words is contains and the frequency table\n",
    "- 4 - Build summary by adding every sentence above a certain score threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop words__ are any word that does not add a value to the meaning of sentence. <br><br>\n",
    "__Corpus__ means a collection of text. <br><br>\n",
    "__Tokenizers__ divides a text into a series of tokens.<br>\n",
    "There are three main tokenizers: word, sentence and regex tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nunes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nunes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work. If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work. If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work. If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work. If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work. If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful. A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text. There are many libraries for NLP. For this project, we will be using NLTK - the Natural Language Toolkit. We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text. In this example, we removed the instances of the words a, in, and the. Corpus means a collection of text. It could be data sets of poems by a certain poet, bodies of work by a certain author, etc. In this case, we are going to use a data set of pre-determined stop words. Basically, it divides a text into a series of tokens. There are three main tokenizers - word, sentence, and regex tokenizer. For this specific project, we will only use the word and sentence tokenizer. So, keep working. Keep striving. Never give up. Fall down seven times, get up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. So, keep moving, keep growing, keep learning. See you at work.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First - create two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one array for stop words \n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "# and other array for every word in the body of the text\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second - create a dictionary for the word frequency table.<br>\n",
    "Words there are not in stopWords array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqTable = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in stopWords:\n",
    "        continue\n",
    "    if word in freqTable:\n",
    "        freqTable[word] += 1\n",
    "    else:\n",
    "        freqTable[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third - assigning a score to every sentence.<br>\n",
    "Building a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentenceValue = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.',\n",
       " 'If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.',\n",
       " 'If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.',\n",
       " 'If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.',\n",
       " 'If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.',\n",
       " 'If you are interested in Data Analytics, you will find learning about Natural Language Processing very useful.',\n",
       " 'A good project to start learning about NLP is to write a summarizer - an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.',\n",
       " 'There are many libraries for NLP.',\n",
       " 'For this project, we will be using NLTK - the Natural Language Toolkit.',\n",
       " 'We usually remove stop words from the analyzed text as knowing their frequency does not give any insight to the body of text.',\n",
       " 'In this example, we removed the instances of the words a, in, and the.',\n",
       " 'Corpus means a collection of text.',\n",
       " 'It could be data sets of poems by a certain poet, bodies of work by a certain author, etc.',\n",
       " 'In this case, we are going to use a data set of pre-determined stop words.',\n",
       " 'Basically, it divides a text into a series of tokens.',\n",
       " 'There are three main tokenizers - word, sentence, and regex tokenizer.',\n",
       " 'For this specific project, we will only use the word and sentence tokenizer.',\n",
       " 'So, keep working.',\n",
       " 'Keep striving.',\n",
       " 'Never give up.',\n",
       " 'Fall down seven times, get up eight.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'Ease is a greater threat to progress than hardship.',\n",
       " 'So, keep moving, keep growing, keep learning.',\n",
       " 'See you at work.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through every sentence and giving it a score depending on the words it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-788d9a7bb015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0msentenceValue\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwordValue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0msentenceValue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordValue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    for wordValue in freqTable:\n",
    "        if wordValue[0] in sentence.lower():\n",
    "            if sentence in sentenceValue:\n",
    "                sentenceValue += wordValue[1]\n",
    "            else:\n",
    "                sentenceValue = wordValue[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
